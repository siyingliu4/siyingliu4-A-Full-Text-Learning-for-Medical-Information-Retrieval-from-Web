{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Testing for all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf, df, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def dic_(allDocument): # alldocument=collection: list\n",
    "    long_str = \" \".join(allDocument)\n",
    "    b=list(set(list(long_str.split())))\n",
    "    return sorted(b)\n",
    "\n",
    "def tf_(doc_que):\n",
    "    counts = Counter(list(doc_que.split()))\n",
    "    return dict(counts)\n",
    "\n",
    "def df_(term, allDocuments):  # df: no. of occurance of a term in whole collection\n",
    "    dic=dict.fromkeys(term, 0)\n",
    "    for i in allDocuments:\n",
    "        n=0\n",
    "        for word in term:\n",
    "            if word in i.split():         \n",
    "                dic[word]=dic[word]+1\n",
    "            n+=1  \n",
    "    return dic  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tfidf_(query_s,c,df,allDocument): # query_s: Series (df['col']), allDocument:list\n",
    "    \n",
    "    all_tfidf=[]\n",
    "    all_tf=[]\n",
    "    for query in query_s:\n",
    "        q_tf=tf_(query)\n",
    "        all_tf.append(q_tf)\n",
    "        mac=q_tf.values()\n",
    "        max_tf=max(q_tf.values())\n",
    "        tfidf=[]\n",
    "        for word in c:\n",
    "            if word not in q_tf: # query item does not shown in doc collection\n",
    "                    value=0\n",
    "            else:\n",
    "                value=(1+math.log10(q_tf.get(word)))/(1+math.log10(max_tf))*math.log10(len(allDocument)/df.get(word))\n",
    "           \n",
    "            tfidf.append(value)\n",
    "        all_tfidf.append(tfidf)\n",
    "    return np.array(all_tfidf), all_tf #vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a,b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_doc=pd.read_excel('df_doc_pre.xlsx')\n",
    "df_query=pd.read_excel('df_query_pre.xlsx')\n",
    "\n",
    "doc_col=df_doc.loc[: , \"Text_tok\"].tolist() #collection: list\n",
    "\n",
    "dic_df=pd.read_csv('dictionary.csv')\n",
    "doc_fre_df=pd.read_csv('document_frequency.csv')\n",
    "dic=dic_df['0'].tolist()\n",
    "doc_fre=pd.Series(doc_fre_df.df.values,index=doc_fre_df.term).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_tfidf,q_tf=tfidf_(df_query[\"Text_tok\"],dic,doc_fre,doc_col) # query tf-idf\n",
    "d_tfidf,d_tf=tfidf_(df_doc[\"Text_tok\"],dic,doc_fre,doc_col) # doc tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# d_tfidf=tfidf_(df_doc[\"Text_tok\"],dic,doc_fre,doc_col) # doc tf-idf\n",
    "# q_tfidf=tfidf_(df_query[\"Text_tok\"],dic,doc_fre,doc_col) # query tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sample query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_query(ran_seed,q_tfidf):\n",
    "    np.random.seed(seed=ran_seed)\n",
    "    idx = np.random.randint(len(q_tfidf), size=1)\n",
    "    q_random=q_tfidf[idx]\n",
    "    return q_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_query_TI(ran_seed,df_query):\n",
    "    np.random.seed(seed=ran_seed)\n",
    "    idx = np.random.randint(len(q_tfidf), size=1)\n",
    "    q_random=df_query.loc[idx]\n",
    "    return q_random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "import time\n",
    "\n",
    "def cosSimilarityTime_Basic(q_tfidf,d_tfidf):\n",
    "    \n",
    "    start = time.time()\n",
    "    sim_all={}\n",
    "    for q in range(len(q_tfidf)):\n",
    "        sim={}\n",
    "        sim_all[q]=sim\n",
    "\n",
    "        for d in range(len(d_tfidf)):\n",
    "            sim[d]=cosine_similarity(d_tfidf[d],q_tfidf[q])\n",
    "    \n",
    "    df_similarity= pd.DataFrame(data=sim_all)\n",
    "    df_similarity['d_index']=df_similarity.index\n",
    "    df1 = df_similarity.reset_index(drop=True)\n",
    "    df2 = pd.melt(df1, id_vars=[\"d_index\"], var_name=\"q_index\", value_name=\"similarity\")\n",
    "    df2=df2.sort_values(by=['q_index','similarity'],ascending=[True,False])\n",
    "\n",
    "    df_query['q_index']=df_query.index\n",
    "    df_doc['d_index']=df_doc.index\n",
    "    df2=pd.merge(df2,df_query[['Query id','q_index']],on='q_index',how='left')\n",
    "    df2=pd.merge(df2,df_doc[['Doc id','d_index']],on='d_index',how='left')\n",
    "    df_similarity_final=df2[['Query id', 'Doc id', 'similarity']]\n",
    "    df_similarity_final=df_similarity_final.dropna()\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiered Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import time\n",
    "\n",
    "a=pd.DataFrame(d_tf).T\n",
    "\n",
    "def top_N(K, dataframe, tiers, n, term_key):\n",
    "# K: the number of top documents\n",
    "    quantile=1\n",
    "    for i in range(tiers):\n",
    "        quantile=quantile-1/tiers\n",
    "        if quantile<0:\n",
    "            quantile=0\n",
    "        quantile_value_list=dataframe.quantile(quantile, axis=1)\n",
    "        s=dataframe.sub(quantile_value_list,axis=0)\n",
    "        s=s.where(s>0,0) \n",
    "        s=s.where(s<=0,1) \n",
    "        columns_sum=s.sum()\n",
    "        top_doc_list=[i for i in range(len(columns_sum)) if columns_sum[i] >= n*len(term_key)]\n",
    "\n",
    "        if len(top_doc_list)>=K:\n",
    "            break\n",
    "    return top_doc_list\n",
    "\n",
    "def tiered_index(K, query_tf, tiers, n):\n",
    "    \n",
    "    top_list=[]\n",
    "    for i in range(len(query_tf)):\n",
    "        term_key=list(query_tf[i].keys())\n",
    "        c=pd.DataFrame(a,index=term_key).fillna(value=0)\n",
    "        List=top_N(K, c, tiers, n, term_key)\n",
    "        top_list.append(List)\n",
    "   \n",
    "    return top_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosSimilarityTime_TieredIndex(q_tfidf,d_tfidf,top_k_list):\n",
    "    start = time.time()\n",
    "    sim_all={}\n",
    "    for q in range(len(q_tfidf)):\n",
    "        sim={}\n",
    "        sim_all[q]=sim\n",
    "        new_d_tfidf=[]\n",
    "        for index in top_k_list[q]:\n",
    "            new_d_tfidf.append(d_tfidf[index])\n",
    "\n",
    "        for d in range(len(new_d_tfidf)):\n",
    "            sim[d+1]=cosine_similarity(new_d_tfidf[d],q_tfidf[q])\n",
    "    \n",
    "    \n",
    "    df_similarity= pd.DataFrame(data=sim_all)\n",
    "    df_similarity['d_index1']=df_similarity.index\n",
    "    df1 = df_similarity.reset_index(drop=True)\n",
    "    df2 = pd.melt(df1, id_vars=[\"d_index1\"], var_name=\"q_index\", value_name=\"similarity\")\n",
    "    df2=df2.dropna()\n",
    "    df2['d_index']=sum(top_k_list, [])\n",
    "    df2 = df2[[\"d_index\",\"q_index\",\"similarity\"]]\n",
    "    df2=df2.sort_values(by=['q_index','similarity'],ascending=[True,False])\n",
    "    df_query['q_index']=df_query.index\n",
    "    df_doc['d_index']=df_doc.index\n",
    "    df2=pd.merge(df2,df_query[['Query id','q_index']],on='q_index',how='left')\n",
    "    df2=pd.merge(df2,df_doc[['Doc id','d_index']],on='d_index',how='left')\n",
    "    df_similarity_final=df2[['Query id', 'Doc id', 'similarity']]\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    return end-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Pre-Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_centroids(points, k, ran_seed):\n",
    "    np.random.seed(seed=ran_seed)\n",
    "    centroids = points.copy()\n",
    "    np.random.shuffle(centroids)\n",
    "    return centroids[:k]\n",
    "\n",
    "def closest_centroid(points, centroids):\n",
    "    cluster_label=[]\n",
    "    for doc in range(len(points)):\n",
    "        similarity=[]\n",
    "        for l in range(len(centroids)):\n",
    "            sim=cosine_similarity(points[doc],centroids[l])\n",
    "            similarity.append(sim)\n",
    "        cluster_label.append(np.argmax(similarity))\n",
    "    return np.asarray(cluster_label)\n",
    "\n",
    "def move_centroids(points, closest, centroids):\n",
    "    mean=[]\n",
    "    for k in range(centroids.shape[0]):\n",
    "        if len(d_tfidf[closest==k])==0:\n",
    "            mean.append([0.5])\n",
    "        else:\n",
    "            mean.append(d_tfidf[closest==k].mean(axis=0))\n",
    "        move_centroid= np.zeros([len(mean),len(max(mean,key = lambda x: len(x)))])\n",
    "        for i,j in enumerate(mean):\n",
    "            move_centroid[i][0:len(j)] = j\n",
    "    return move_centroid\n",
    "    #return np.array([points[closest==k].mean(axis=0) for k in range(centroids.shape[0])])\n",
    "    \n",
    "def k_means(points, k,ran_seed):\n",
    "    leaders=initialize_centroids(points, k, ran_seed)\n",
    "    move=True\n",
    "    while move != False:\n",
    "        leaders_new=move_centroids(points, closest_centroid(points, leaders), leaders)\n",
    "        for i in range(len(leaders)):\n",
    "            if cosine_similarity(leaders[i],leaders_new[i])>0.99999:\n",
    "                move=False\n",
    "            else:\n",
    "                move=True\n",
    "        leaders=leaders_new\n",
    "    return leaders, closest_centroid(points, leaders) # cluster label\n",
    "\n",
    "def pre_clustering(doc,k,ran_seed):\n",
    "    cluster=k_means(doc, k, ran_seed)\n",
    "    leader=cluster[0]\n",
    "    cluster_label=cluster[1]\n",
    "    df_d_tfidf = pd.DataFrame(doc)\n",
    "    df_d_tfidf['label']=cluster_label\n",
    "    return leader, df_d_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "import time\n",
    "\n",
    "def cosSimilarityTime_kmeans(q_tfidf,leader,df_d_tfidf):\n",
    "    \n",
    "    start = time.time()\n",
    "    sim_all={}\n",
    "    for q in range(len(q_tfidf)):\n",
    "        leader_sim={}\n",
    "        sim={}\n",
    "        sim_all[q]=sim\n",
    "\n",
    "        for l in range(len(leader)):\n",
    "            leader_sim[l]=cosine_similarity(leader[l],q_tfidf[q])\n",
    "\n",
    "        closest_cluster_label=max(leader_sim.items(), key=operator.itemgetter(1))[0]\n",
    "        closest_cluster=df_d_tfidf.loc[df_d_tfidf['label'] == closest_cluster_label].drop(['label'],axis=1)\n",
    "\n",
    "        for d in closest_cluster.index:\n",
    "            sim[d]=cosine_similarity(closest_cluster.loc[d],q_tfidf[q])\n",
    "\n",
    "\n",
    "    df_similarity= pd.DataFrame(data=sim_all)\n",
    "    df_similarity['d_index']=df_similarity.index\n",
    "    df1 = df_similarity.reset_index(drop=True)\n",
    "    df2 = pd.melt(df1, id_vars=[\"d_index\"], var_name=\"q_index\", value_name=\"similarity\")\n",
    "    df2=df2.sort_values(by=['q_index','similarity'],ascending=[True,False])\n",
    "\n",
    "    df_query['q_index']=df_query.index\n",
    "    df_doc['d_index']=df_doc.index\n",
    "    df2=pd.merge(df2,df_query[['Query id','q_index']],on='q_index',how='left')\n",
    "    df2=pd.merge(df2,df_doc[['Doc id','d_index']],on='d_index',how='left')\n",
    "    df_similarity_final=df2[['Query id', 'Doc id', 'similarity']]\n",
    "    df_similarity_final=df_similarity_final.dropna()\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster=pre_clustering(d_tfidf,300,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_doc_rp=pd.read_csv('d_tfidf2000.csv')\n",
    "df_query_rp=pd.read_csv('q_tfidf2000.csv')\n",
    "\n",
    "df_doc_rp=df_doc_rp.drop(['Unnamed: 0'],axis=1)\n",
    "df_query_rp=df_query_rp.drop(['Unnamed: 0'],axis=1)\n",
    "\n",
    "d_tfidf_rp=df_doc_rp.as_matrix(columns=None)\n",
    "q_tfidf_rp=df_query_rp.as_matrix(columns=None)\n",
    "\n",
    "#df_query_origin=pd.read_excel('df_query_pre.xlsx')\n",
    "df_query_rp=pd.merge(df_query_rp,df_query[['Query id']], left_index=True, right_index=True,how='left')\n",
    "#df_doc_origin=pd.read_excel('df_doc_pre.xlsx')\n",
    "df_doc_rp=pd.merge(df_doc_rp,df_doc[['Doc id']], left_index=True, right_index=True,how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def hammingSimilarity(a,b):\n",
    "    similarity = np.sum(a == b)\n",
    "    return similarity\n",
    "\n",
    "def hammingSimilarityTime_RP(q_tfidf,d_tfidf):\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    sim_all={}\n",
    "    for q in range(len(q_tfidf)):\n",
    "        sim={}\n",
    "        sim_all[q]=sim\n",
    "    \n",
    "        for d in range(len(d_tfidf)):\n",
    "            sim[d]=hammingSimilarity(d_tfidf[d],q_tfidf[q])\n",
    "        \n",
    "    \n",
    "    df_similarity= pd.DataFrame(data=sim_all)\n",
    "    df_similarity['d_index']=df_similarity.index\n",
    "    df1 = df_similarity.reset_index(drop=True)\n",
    "    df2 = pd.melt(df1, id_vars=[\"d_index\"], var_name=\"q_index\", value_name=\"similarity\")\n",
    "    df2=df2.sort_values(by=['q_index','similarity'],ascending=[True,False])\n",
    "    df_query_rp['q_index']=df_query_rp.index\n",
    "    df_doc_rp['d_index']=df_doc_rp.index\n",
    "    df2=pd.merge(df2,df_query_rp[['Query id','q_index']],on='q_index',how='left')\n",
    "    df2=pd.merge(df2,df_doc_rp[['Doc id','d_index']],on='d_index',how='left')\n",
    "    df_similarity_final=df2[['Query id', 'Doc id', 'similarity']]\n",
    "    \n",
    "    end = time.time()\n",
    "\n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ranking time per query:\n",
      "Basic Model: 0.261049962044\n",
      "Tiered Index: 0.0296452057362\n",
      "Pre-Clustering: 0.0345848608017\n",
      "Random Projection: 0.0918842077255\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print (\"Average ranking time per query:\")\n",
    "\n",
    "# Basic Model\n",
    "seed=list(range(0,200))\n",
    "time_pool=[]\n",
    "for i in seed:\n",
    "    qequey_for_time=random_query(i,q_tfidf)\n",
    "    query_time=cosSimilarityTime_Basic(qequey_for_time,d_tfidf)\n",
    "    time_pool.append(query_time)\n",
    "    \n",
    "print (\"Basic Model:\",np.mean(time_pool))\n",
    "\n",
    "# Tiered Index\n",
    "seed=list(range(0,200))\n",
    "time_pool=[]\n",
    "for i in seed:\n",
    "    qequey_for_time=random_query_TI(i,df_query)\n",
    "    q_tfidf,q_tf=tfidf_(qequey_for_time[\"Text_tok\"],dic,doc_fre,doc_col) # query tf-idf  \n",
    "    \n",
    "    top_k_list=tiered_index(50, q_tf, 3, 0.06)\n",
    "    query_time=cosSimilarityTime_TieredIndex(q_tfidf,d_tfidf,top_k_list)\n",
    "    time_pool.append(query_time)\n",
    "    \n",
    "print (\"Tiered Index:\",np.mean(time_pool))\n",
    "\n",
    "# Pre-Clustering\n",
    "seed=list(range(0,200))\n",
    "time_pool=[]\n",
    "for i in seed:\n",
    "    qequey_for_time=random_query(i,q_tfidf)\n",
    "    query_time=cosSimilarityTime_kmeans(qequey_for_time,cluster[0],cluster[1])\n",
    "    time_pool.append(query_time)\n",
    "    \n",
    "print (\"Pre-Clustering:\",np.mean(time_pool))\n",
    "\n",
    "# Random Projection\n",
    "seed=list(range(0,200))\n",
    "time_pool=[]\n",
    "for i in seed:\n",
    "    qequey_for_time=random_query(i,q_tfidf_rp)\n",
    "    query_time=hammingSimilarityTime_RP(qequey_for_time,q_tfidf_rp)\n",
    "    time_pool.append(query_time)\n",
    "    \n",
    "print (\"Random Projection:\",np.mean(time_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
