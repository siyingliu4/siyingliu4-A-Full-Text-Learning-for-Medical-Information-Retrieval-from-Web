{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF schema0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf, df, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def dic_(allDocument): # alldocument=collection: list\n",
    "    long_str = \" \".join(allDocument)\n",
    "    b=list(set(list(long_str.split())))\n",
    "    return sorted(b)\n",
    "\n",
    "def tf_(doc_que):\n",
    "    counts = Counter(list(doc_que.split()))\n",
    "    return dict(counts)\n",
    "\n",
    "def df_(term, allDocuments):  # df: no. of occurance of a term in whole collection\n",
    "    dic=dict.fromkeys(term, 0)\n",
    "    for i in allDocuments:\n",
    "        n=0\n",
    "        for word in term:\n",
    "            if word in i.split():         \n",
    "                dic[word]=dic[word]+1\n",
    "            n+=1  \n",
    "    return dic  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tfidf_(query_s,c,df,allDocument): # query_s: Series (df['col']), allDocument:list\n",
    "\n",
    "    all_tfidf=[]\n",
    "    for query in query_s:\n",
    "        q_tf=tf_(query)\n",
    "        max_tf=max(q_tf.values())\n",
    "        tfidf=[]\n",
    "        for word in c:\n",
    "            if word not in q_tf: # query item does not shown in doc collection\n",
    "                    value=0\n",
    "            else:\n",
    "                value=(1+math.log10(q_tf.get(word)))/(1+math.log10(max_tf))*math.log10(len(allDocument)/df.get(word))\n",
    "            tfidf.append(value)\n",
    "        all_tfidf.append(tfidf)\n",
    "    return np.array(all_tfidf) #vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cosine_similarity(a,b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Projection_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_doc=pd.read_csv('d_tfidf2000.csv')\n",
    "df_query=pd.read_csv('q_tfidf2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doc=df_doc.drop(['Unnamed: 0'],axis=1)\n",
    "df_query=df_query.drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tfidf=df_doc.as_matrix(columns=None)\n",
    "q_tfidf=df_query.as_matrix(columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_query_origin=pd.read_excel('df_query_pre.xlsx')\n",
    "df_query=pd.merge(df_query,df_query_origin[['Query id']], left_index=True, right_index=True,how='left')\n",
    "df_doc_origin=pd.read_excel('df_doc_pre.xlsx')\n",
    "df_doc=pd.merge(df_doc,df_doc_origin[['Doc id']], left_index=True, right_index=True,how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_centroids(points, k, ran_seed):\n",
    "    np.random.seed(seed=ran_seed)\n",
    "    centroids = points.copy()\n",
    "    np.random.shuffle(centroids)\n",
    "    return centroids[:k]\n",
    "\n",
    "def closest_centroid(points, centroids):\n",
    "    cluster_label=[]\n",
    "    for doc in range(len(points)):\n",
    "        similarity=[]\n",
    "        for l in range(len(centroids)):\n",
    "            sim=cosine_similarity(points[doc],centroids[l])\n",
    "            similarity.append(sim)\n",
    "        cluster_label.append(np.argmax(similarity))\n",
    "    return np.asarray(cluster_label)\n",
    "\n",
    "def move_centroids(points, closest, centroids):\n",
    "    mean=[]\n",
    "    for k in range(centroids.shape[0]):\n",
    "        if len(d_tfidf[closest==k])==0:\n",
    "            mean.append([0.5])\n",
    "        else:\n",
    "            mean.append(d_tfidf[closest==k].mean(axis=0))\n",
    "        move_centroid= np.zeros([len(mean),len(max(mean,key = lambda x: len(x)))])\n",
    "        for i,j in enumerate(mean):\n",
    "            move_centroid[i][0:len(j)] = j\n",
    "    return move_centroid\n",
    "    #return np.array([points[closest==k].mean(axis=0) for k in range(centroids.shape[0])])\n",
    "    \n",
    "def k_means(points, k,ran_seed):\n",
    "    leaders=initialize_centroids(points, k, ran_seed)\n",
    "    move=True\n",
    "    while move != False:\n",
    "        leaders_new=move_centroids(points, closest_centroid(points, leaders), leaders)\n",
    "        for i in range(len(leaders)):\n",
    "            if cosine_similarity(leaders[i],leaders_new[i])>0.99999:\n",
    "                move=False\n",
    "            else:\n",
    "                move=True\n",
    "        leaders=leaders_new\n",
    "    return leaders, closest_centroid(points, leaders) # cluster label\n",
    "\n",
    "def pre_clustering(doc,k,ran_seed):\n",
    "    cluster=k_means(doc, k, ran_seed)\n",
    "    leader=cluster[0]\n",
    "    cluster_label=cluster[1]\n",
    "    df_d_tfidf = pd.DataFrame(doc)\n",
    "    df_d_tfidf['label']=cluster_label\n",
    "    return leader, df_d_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "import time\n",
    "\n",
    "def cosSimilarityImp(q_tfidf,leader, df_d_tfidf):\n",
    "    \n",
    "    start = time.time()\n",
    "    sim_all={}\n",
    "    for q in range(len(q_tfidf)):\n",
    "        leader_sim={}\n",
    "        sim={}\n",
    "        sim_all[q]=sim\n",
    "\n",
    "        for l in range(len(leader)):\n",
    "            leader_sim[l]=cosine_similarity(leader[l],q_tfidf[q])\n",
    "\n",
    "        closest_cluster_label=max(leader_sim.items(), key=operator.itemgetter(1))[0]\n",
    "        closest_cluster=df_d_tfidf.loc[df_d_tfidf['label'] == closest_cluster_label].drop(['label'],axis=1)\n",
    "\n",
    "        for d in closest_cluster.index:\n",
    "            sim[d]=cosine_similarity(closest_cluster.loc[d],q_tfidf[q])\n",
    "\n",
    "    end = time.time()\n",
    "    print('similarity time: ',end - start)\n",
    "\n",
    "    df_similarity= pd.DataFrame(data=sim_all)\n",
    "    df_similarity['d_index']=df_similarity.index\n",
    "    df1 = df_similarity.reset_index(drop=True)\n",
    "    df2 = pd.melt(df1, id_vars=[\"d_index\"], var_name=\"q_index\", value_name=\"similarity\")\n",
    "    df2=df2.sort_values(by=['q_index','similarity'],ascending=[True,False])\n",
    "\n",
    "    df_query['q_index']=df_query.index\n",
    "    df_doc['d_index']=df_doc.index\n",
    "    df2=pd.merge(df2,df_query[['Query id','q_index']],on='q_index',how='left')\n",
    "    df2=pd.merge(df2,df_doc[['Doc id','d_index']],on='d_index',how='left')\n",
    "    df_similarity_final=df2[['Query id', 'Doc id', 'similarity']]\n",
    "    df_similarity_final=df_similarity_final.dropna()\n",
    "\n",
    "    # measuremnt format\n",
    "    rel_label = pd.read_excel(\"all2-1-0.qrel.xlsx\")\n",
    "    rel_similarity=pd.merge(df_similarity_final[['Query id','Doc id']],rel_label, on=['Query id','Doc id'], how='left')\n",
    "    rel_similarity=rel_similarity.fillna(value=0)\n",
    "    rel_similarity=rel_similarity.drop(['Doc id'],axis=1)\n",
    "    s = rel_similarity.groupby('Query id')['Rel_level'].apply(lambda x: x.tolist())\n",
    "    correct_input=s.values\n",
    "\n",
    "    return correct_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "# presicion \n",
    "def precision(y_true):\n",
    "#y_true: this list reordered y_true list\n",
    "    p=[]\n",
    "    a=0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i]==1:\n",
    "            a+=1\n",
    "            precision=a/(i+1)\n",
    "            p.append(precision)\n",
    "    #print(p)\n",
    "    return p\n",
    "\n",
    "# average precision\n",
    "def AP(y_true):\n",
    "    p=precision(y_true)\n",
    "    if len(p)!=0:\n",
    "        AP=sum(p)/len(p)\n",
    "    else:\n",
    "        AP=0\n",
    "    #print(AP)\n",
    "    return AP\n",
    "\n",
    "\n",
    "def MAP(list_true):# query list \n",
    "    ap=[]\n",
    "    for i in range(len(list_true)):\n",
    "        ap.append(AP(list_true[i]))\n",
    "    total=sum(ap)\n",
    "    #print(ap)\n",
    "    return total/(len(list_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nDCG1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def ndcg1(correct):\n",
    "    if sum(correct)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        dcg=0\n",
    "        for i in range(len(correct)):\n",
    "            gain=2**correct[i]-1\n",
    "            discounts=np.log2(i+2)\n",
    "            dcg=dcg+gain/discounts\n",
    "    \n",
    "        #idcg\n",
    "        order = np.argsort(correct)\n",
    "        sort_correct = np.take(correct, order[::-1])\n",
    "        idcg=0\n",
    "        for i in range(len(sort_correct)):\n",
    "            gain=2**sort_correct[i]-1\n",
    "            discounts=np.log2(i+2)\n",
    "            idcg=idcg+gain/discounts\n",
    "        #print(dcg)\n",
    "        #print(idcg)\n",
    "        return dcg/idcg\n",
    "\n",
    "def mean_ndcg1(correct_list):\n",
    "    b=0\n",
    "    for correct in correct_list:\n",
    "        a=ndcg1(correct)\n",
    "        b=b+a\n",
    "    return b/len(correct_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leader size: 300\n",
      "similarity time:  95.25399971008301\n",
      "MAP: 0.08799739678910269\n",
      "nDCG: 0.153992917227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leader_size=[300]\n",
    "for i in leader_size:\n",
    "    print('leader size:',i)\n",
    "    cluster=pre_clustering(d_tfidf,i,0)\n",
    "    result=cosSimilarityImp(q_tfidf,cluster[0], cluster[1])\n",
    "    print('MAP:',MAP(result)) \n",
    "    print('nDCG:',mean_ndcg1(result))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
