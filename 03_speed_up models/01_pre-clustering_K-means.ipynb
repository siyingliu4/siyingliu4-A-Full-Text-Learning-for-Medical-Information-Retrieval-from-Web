{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-clustering with K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf, df, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def dic_(allDocument): # alldocument=collection: list\n",
    "    long_str = \" \".join(allDocument)\n",
    "    b=list(set(list(long_str.split())))\n",
    "    return sorted(b)\n",
    "\n",
    "def tf_(doc_que): # term frequency\n",
    "    counts = Counter(list(doc_que.split()))\n",
    "    return dict(counts)\n",
    "\n",
    "def df_(term, allDocuments):  # df: no. of occurance of a term in whole collection\n",
    "    dic=dict.fromkeys(term, 0)\n",
    "    for i in allDocuments:\n",
    "        n=0\n",
    "        for word in term:\n",
    "            if word in i.split():         \n",
    "                dic[word]=dic[word]+1\n",
    "            n+=1  \n",
    "    return dic  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tfidf_(query_s,c,df,allDocument): # query_s: Series (df['col']), allDocument:list\n",
    "\n",
    "    all_tfidf=[]\n",
    "    for query in query_s:\n",
    "        q_tf=tf_(query)\n",
    "        max_tf=max(q_tf.values())\n",
    "        tfidf=[]\n",
    "        for word in c:\n",
    "            if word not in q_tf: # query item does not shown in doc collection\n",
    "                    value=0\n",
    "            else:\n",
    "                value=(1+math.log10(q_tf.get(word)))/(1+math.log10(max_tf))*math.log10(len(allDocument)/df.get(word))\n",
    "            tfidf.append(value)\n",
    "        all_tfidf.append(tfidf)\n",
    "    return np.array(all_tfidf) #vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a,b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_doc=pd.read_excel('df_doc_pre.xlsx')\n",
    "df_query=pd.read_excel('df_query_pre.xlsx')\n",
    "doc_col=df_doc.loc[: , \"Text_tok\"].tolist() #collection: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic_df=pd.read_csv('dictionary.csv')\n",
    "doc_fre_df=pd.read_csv('document_frequency.csv')\n",
    "dic=dic_df['0'].tolist()\n",
    "doc_fre=pd.Series(doc_fre_df.df.values,index=doc_fre_df.term).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tfidf=tfidf_(df_doc[\"Text_tok\"],dic,doc_fre,doc_col) # doc tf-idf\n",
    "q_tfidf=tfidf_(df_query[\"Text_tok\"],dic,doc_fre,doc_col) # query tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# randomly select initial doc leaders\n",
    "def initialize_centroids(points, k, ran_seed):\n",
    "    np.random.seed(seed=ran_seed)\n",
    "    centroids = points.copy()\n",
    "    np.random.shuffle(centroids)\n",
    "    return centroids[:k]\n",
    "\n",
    "# assign each doc to cloest leader cluster\n",
    "def closest_centroid(points, centroids):\n",
    "    cluster_label=[]\n",
    "    for doc in range(len(points)):\n",
    "        similarity=[]\n",
    "        for l in range(len(centroids)):\n",
    "            sim=cosine_similarity(points[doc],centroids[l])\n",
    "            similarity.append(sim)\n",
    "        cluster_label.append(np.argmax(similarity))\n",
    "    return np.asarray(cluster_label)\n",
    "\n",
    "# adjust doc leaders by take centroid of initial cluster\n",
    "def move_centroids(points, closest, centroids):\n",
    "    mean=[]\n",
    "    for k in range(centroids.shape[0]):\n",
    "        if len(d_tfidf[closest==k])==0:\n",
    "            mean.append([0.5])\n",
    "        else:\n",
    "            mean.append(d_tfidf[closest==k].mean(axis=0))\n",
    "        move_centroid= np.zeros([len(mean),len(max(mean,key = lambda x: len(x)))])\n",
    "        for i,j in enumerate(mean):\n",
    "            move_centroid[i][0:len(j)] = j\n",
    "    return move_centroid\n",
    "    #return np.array([points[closest==k].mean(axis=0) for k in range(centroids.shape[0])])\n",
    "\n",
    "# recurrently adjust cluster until no movement among clusters\n",
    "def k_means(points, k,ran_seed):\n",
    "    leaders=initialize_centroids(points, k, ran_seed)\n",
    "    move=True\n",
    "    while move != False:\n",
    "        leaders_new=move_centroids(points, closest_centroid(points, leaders), leaders)\n",
    "        for i in range(len(leaders)):\n",
    "            if cosine_similarity(leaders[i],leaders_new[i])>0.99999:\n",
    "                move=False\n",
    "            else:\n",
    "                move=True\n",
    "        leaders=leaders_new\n",
    "    return leaders, closest_centroid(points, leaders) # cluster label\n",
    "\n",
    "# integrated call k-means for all doc file\n",
    "def pre_clustering(doc,k,ran_seed):\n",
    "    cluster=k_means(doc, k, ran_seed)\n",
    "    leader=cluster[0]\n",
    "    cluster_label=cluster[1]\n",
    "    df_d_tfidf = pd.DataFrame(doc)\n",
    "    df_d_tfidf['label']=cluster_label\n",
    "    return leader, df_d_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation: Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "import time\n",
    "\n",
    "def cosSimilarityImp(q_tfidf,leader, df_d_tfidf):\n",
    "    \n",
    "    start = time.time()\n",
    "    sim_all={}\n",
    "    for q in range(len(q_tfidf)):\n",
    "        leader_sim={}\n",
    "        sim={}\n",
    "        sim_all[q]=sim\n",
    "        \n",
    "        # compare similarity between query and doc leaders\n",
    "        for l in range(len(leader)):\n",
    "            leader_sim[l]=cosine_similarity(leader[l],q_tfidf[q])\n",
    "        \n",
    "        # access the cloest leader's cluster\n",
    "        closest_cluster_label=max(leader_sim.items(), key=operator.itemgetter(1))[0]\n",
    "        closest_cluster=df_d_tfidf.loc[df_d_tfidf['label'] == closest_cluster_label].drop(['label'],axis=1)\n",
    "        \n",
    "        # calculate similarity amongth docs in the cloest cluster\n",
    "        for d in closest_cluster.index:\n",
    "            sim[d]=cosine_similarity(closest_cluster.loc[d],q_tfidf[q])\n",
    "\n",
    "    end = time.time()\n",
    "    print('similarity time: ',end - start)\n",
    "\n",
    "    # ranking doc by similartiy\n",
    "    # covert format to a table\n",
    "    df_similarity= pd.DataFrame(data=sim_all)\n",
    "    df_similarity['d_index']=df_similarity.index\n",
    "    df1 = df_similarity.reset_index(drop=True)\n",
    "    df2 = pd.melt(df1, id_vars=[\"d_index\"], var_name=\"q_index\", value_name=\"similarity\")\n",
    "    df2=df2.sort_values(by=['q_index','similarity'],ascending=[True,False])\n",
    "    \n",
    "    # prepare format for meaturement\n",
    "    df_query['q_index']=df_query.index\n",
    "    df_doc['d_index']=df_doc.index\n",
    "    df2=pd.merge(df2,df_query[['Query id','q_index']],on='q_index',how='left')\n",
    "    df2=pd.merge(df2,df_doc[['Doc id','d_index']],on='d_index',how='left')\n",
    "    df_similarity_final=df2[['Query id', 'Doc id', 'similarity']]\n",
    "    df_similarity_final=df_similarity_final.dropna()\n",
    "\n",
    "    # measuremnt format\n",
    "    rel_label = pd.read_excel(\"all2-1-0.qrel.xlsx\")\n",
    "    rel_similarity=pd.merge(df_similarity_final[['Query id','Doc id']],rel_label, on=['Query id','Doc id'], how='left')\n",
    "    rel_similarity=rel_similarity.fillna(value=0)\n",
    "    rel_similarity=rel_similarity.drop(['Doc id'],axis=1)\n",
    "    s = rel_similarity.groupby('Query id')['Rel_level'].apply(lambda x: x.tolist())\n",
    "    correct_input=s.values\n",
    "\n",
    "    return correct_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "# presicion \n",
    "def precision(y_true):\n",
    "#y_true: this list reordered y_true list\n",
    "    p=[]\n",
    "    a=0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i]==1:\n",
    "            a+=1\n",
    "            precision=a/(i+1)\n",
    "            p.append(precision)\n",
    "    #print(p)\n",
    "    return p\n",
    "\n",
    "# average precision\n",
    "def AP(y_true):\n",
    "    p=precision(y_true)\n",
    "    if len(p)!=0:\n",
    "        AP=sum(p)/len(p)\n",
    "    else:\n",
    "        AP=0\n",
    "    #print(AP)\n",
    "    return AP\n",
    "\n",
    "\n",
    "def MAP(list_true):# query list \n",
    "    ap=[]\n",
    "    for i in range(len(list_true)):\n",
    "        ap.append(AP(list_true[i]))\n",
    "    total=sum(ap)\n",
    "    #print(ap)\n",
    "    return total/(len(list_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nDCG1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def ndcg1(correct):\n",
    "    if sum(correct)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        dcg=0\n",
    "        for i in range(len(correct)):\n",
    "            gain=2**correct[i]-1\n",
    "            discounts=np.log2(i+2)\n",
    "            dcg=dcg+gain/discounts\n",
    "    \n",
    "        #idcg\n",
    "        order = np.argsort(correct)\n",
    "        sort_correct = np.take(correct, order[::-1])\n",
    "        idcg=0\n",
    "        for i in range(len(sort_correct)):\n",
    "            gain=2**sort_correct[i]-1\n",
    "            discounts=np.log2(i+2)\n",
    "            idcg=idcg+gain/discounts\n",
    "        #print(dcg)\n",
    "        #print(idcg)\n",
    "        return dcg/idcg\n",
    "\n",
    "def mean_ndcg1(correct_list):\n",
    "    b=0\n",
    "    for correct in correct_list:\n",
    "        a=ndcg1(correct)\n",
    "        b=b+a\n",
    "    return b/len(correct_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leader size: 73\n",
      "similarity time:  130.38399982452393\n",
      "MAP: 0.16647725211744085\n",
      "nDCG: 0.332509782158\n",
      "\n",
      "leader size: 10\n",
      "similarity time:  713.3910000324249\n",
      "MAP: 0.14071103444505317\n",
      "nDCG: 0.366328603313\n",
      "\n",
      "leader size: 20\n",
      "similarity time:  398.2190001010895\n",
      "MAP: 0.15549867223332753\n",
      "nDCG: 0.371494921929\n",
      "\n",
      "leader size: 30\n",
      "similarity time:  263.15000009536743\n",
      "MAP: 0.14598624816610853\n",
      "nDCG: 0.32956926687\n",
      "\n",
      "leader size: 40\n",
      "similarity time:  208.57400012016296\n",
      "MAP: 0.1499140295206088\n",
      "nDCG: 0.324891392085\n",
      "\n",
      "leader size: 50\n",
      "similarity time:  203.88299989700317\n",
      "MAP: 0.1608903085845128\n",
      "nDCG: 0.333404291404\n",
      "\n",
      "leader size: 60\n",
      "similarity time:  159.4539999961853\n",
      "MAP: 0.1706197191414928\n",
      "nDCG: 0.343818173453\n",
      "\n",
      "leader size: 70\n",
      "similarity time:  143.66100001335144\n",
      "MAP: 0.1677409713368592\n",
      "nDCG: 0.336407876822\n",
      "\n",
      "leader size: 80\n",
      "similarity time:  122.69800019264221\n",
      "MAP: 0.16873337677021938\n",
      "nDCG: 0.334669620124\n",
      "\n",
      "leader size: 90\n",
      "similarity time:  118.66099977493286\n",
      "MAP: 0.17147095218919614\n",
      "nDCG: 0.338990584298\n",
      "\n",
      "leader size: 100\n",
      "similarity time:  111.50499987602234\n",
      "MAP: 0.1691981054398112\n",
      "nDCG: 0.335787532859\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leader_size=[round(math.sqrt(len(df_doc))),10,20,30,40,50,60,70,80,90,100]\n",
    "for i in leader_size:\n",
    "    print('leader size:',i)\n",
    "    cluster=pre_clustering(d_tfidf,i,0)\n",
    "    result=cosSimilarityImp(q_tfidf,cluster[0], cluster[1])\n",
    "    print('MAP:',MAP(result)) \n",
    "    print('nDCG:',mean_ndcg1(result))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leader size: 110\n",
      "similarity time:  106.97599983215332\n",
      "MAP: 0.17466469715793795\n",
      "nDCG: 0.340284182768\n",
      "\n",
      "leader size: 120\n",
      "similarity time:  104.0990002155304\n",
      "MAP: 0.17644108208323311\n",
      "nDCG: 0.341795856724\n",
      "\n",
      "leader size: 130\n",
      "similarity time:  89.58999991416931\n",
      "MAP: 0.17500583360650274\n",
      "nDCG: 0.334936568678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leader_size=[110,120,130]\n",
    "for i in leader_size:\n",
    "    print('leader size:',i)\n",
    "    cluster=pre_clustering(d_tfidf,i,0)\n",
    "    result=cosSimilarityImp(q_tfidf,cluster[0], cluster[1])\n",
    "    print('MAP:',MAP(result)) \n",
    "    print('nDCG:',mean_ndcg1(result))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leader size: 140\n",
      "similarity time:  88.0\n",
      "MAP: 0.17942545037657873\n",
      "nDCG: 0.339927571527\n",
      "\n",
      "leader size: 150\n",
      "similarity time:  85.20500016212463\n",
      "MAP: 0.1877172936437659\n",
      "nDCG: 0.349451203786\n",
      "\n",
      "leader size: 160\n",
      "similarity time:  153.2464997768402\n",
      "MAP: 0.1861306809915443\n",
      "nDCG: 0.335513375168\n",
      "\n",
      "leader size: 170\n",
      "similarity time:  80.12099981307983\n",
      "MAP: 0.17900668129252661\n",
      "nDCG: 0.323470708261\n",
      "\n",
      "leader size: 180\n",
      "similarity time:  75.92799997329712\n",
      "MAP: 0.18895744958751873\n",
      "nDCG: 0.340065755626\n",
      "\n",
      "leader size: 190\n",
      "similarity time:  99.0770001411438\n",
      "MAP: 0.1918335052238354\n",
      "nDCG: 0.350583086344\n",
      "\n",
      "leader size: 200\n",
      "similarity time:  97.99000000953674\n",
      "MAP: 0.19413169712082684\n",
      "nDCG: 0.355728402021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leader_size=[140,150,160,170,180,190,200]\n",
    "for i in leader_size:\n",
    "    print('leader size:',i)\n",
    "    cluster=pre_clustering(d_tfidf,i,0)\n",
    "    result=cosSimilarityImp(q_tfidf,cluster[0], cluster[1])\n",
    "    print('MAP:',MAP(result)) \n",
    "    print('nDCG:',mean_ndcg1(result))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leader size: 210\n",
      "similarity time:  84.76899981498718\n",
      "MAP: 0.18903357735855497\n",
      "nDCG: 0.344107942425\n",
      "\n",
      "leader size: 220\n",
      "similarity time:  84.1470000743866\n",
      "MAP: 0.1901799031129375\n",
      "nDCG: 0.346668773164\n",
      "\n",
      "leader size: 230\n",
      "similarity time:  85.21300029754639\n",
      "MAP: 0.19647131914645843\n",
      "nDCG: 0.356512201186\n",
      "\n",
      "leader size: 240\n",
      "similarity time:  83.21600008010864\n",
      "MAP: 0.19953938571476806\n",
      "nDCG: 0.356800576444\n",
      "\n",
      "leader size: 250\n",
      "similarity time:  86.43899989128113\n",
      "MAP: 0.19399494724555605\n",
      "nDCG: 0.351518256047\n",
      "\n",
      "leader size: 260\n",
      "similarity time:  84.73699998855591\n",
      "MAP: 0.19198471706786763\n",
      "nDCG: 0.346114078491\n",
      "\n",
      "leader size: 270\n",
      "similarity time:  84.90200018882751\n",
      "MAP: 0.19919271505162595\n",
      "nDCG: 0.356598269705\n",
      "\n",
      "leader size: 280\n",
      "similarity time:  83.6159999370575\n",
      "MAP: 0.19598083242870845\n",
      "nDCG: 0.351317699869\n",
      "\n",
      "leader size: 290\n",
      "similarity time:  69.45799994468689\n",
      "MAP: 0.19772726226067341\n",
      "nDCG: 0.350784532249\n",
      "\n",
      "leader size: 300\n",
      "similarity time:  71.48499989509583\n",
      "MAP: 0.20065772769139037\n",
      "nDCG: 0.357306195665\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leader_size=[210,220,230,240,250,260,270,280,290,300]\n",
    "for i in leader_size:\n",
    "    print('leader size:',i)\n",
    "    cluster=pre_clustering(d_tfidf,i,0)\n",
    "    result=cosSimilarityImp(q_tfidf,cluster[0], cluster[1])\n",
    "    print('MAP:',MAP(result)) \n",
    "    print('nDCG:',mean_ndcg1(result))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leader size: 310\n",
      "similarity time:  66.46840023994446\n",
      "MAP: 0.19995321502849656\n",
      "nDCG: 0.359862948451\n",
      "\n",
      "leader size: 320\n",
      "similarity time:  64.26240038871765\n",
      "MAP: 0.2001392196866568\n",
      "nDCG: 0.356806226464\n",
      "\n",
      "leader size: 330\n",
      "similarity time:  67.57520008087158\n",
      "MAP: 0.2018556793308341\n",
      "nDCG: 0.358794050847\n",
      "\n",
      "leader size: 340\n",
      "similarity time:  68.06320023536682\n",
      "MAP: 0.20453057735783822\n",
      "nDCG: 0.361440821342\n",
      "\n",
      "leader size: 350\n",
      "similarity time:  67.9992003440857\n",
      "MAP: 0.2086312621173508\n",
      "nDCG: 0.364418089138\n",
      "\n",
      "leader size: 360\n",
      "similarity time:  70.13760042190552\n",
      "MAP: 0.20571362472854066\n",
      "nDCG: 0.36191018093\n",
      "\n",
      "leader size: 370\n",
      "similarity time:  66.26280045509338\n",
      "MAP: 0.20899772728906782\n",
      "nDCG: 0.36500915294\n",
      "\n",
      "leader size: 380\n",
      "similarity time:  67.4558002948761\n",
      "MAP: 0.2123166904399022\n",
      "nDCG: 0.369244856697\n",
      "\n",
      "leader size: 390\n",
      "similarity time:  71.2746000289917\n",
      "MAP: 0.2129859843112712\n",
      "nDCG: 0.368860704528\n",
      "\n",
      "leader size: 400\n",
      "similarity time:  70.76460027694702\n",
      "MAP: 0.2132481427708949\n",
      "nDCG: 0.367020541802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leader_size=[310,320,330,340,350,360,370,380,390,400]\n",
    "for i in leader_size:\n",
    "    print('leader size:',i)\n",
    "    cluster=pre_clustering(d_tfidf,i,0)\n",
    "    result=cosSimilarityImp(q_tfidf,cluster[0], cluster[1])\n",
    "    print('MAP:',MAP(result)) \n",
    "    print('nDCG:',mean_ndcg1(result))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leader size: 1000\n",
      "similarity time:  109.51120042800903\n",
      "MAP: 0.23940693775627106\n",
      "nDCG: 0.385937226586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leader_size=[1000]\n",
    "for i in leader_size:\n",
    "    print('leader size:',i)\n",
    "    cluster=pre_clustering(d_tfidf,i,0)\n",
    "    result=cosSimilarityImp(q_tfidf,cluster[0], cluster[1])\n",
    "    print('MAP:',MAP(result)) \n",
    "    print('nDCG:',mean_ndcg1(result))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leader size: 2000\n",
      "similarity time:  199.09560084342957\n",
      "MAP: 0.2709207573606207\n",
      "nDCG: 0.423240905447\n",
      "\n",
      "leader size: 3000\n",
      "similarity time:  287.1690020561218\n",
      "MAP: 0.2641303910279825\n",
      "nDCG: 0.418096768901\n",
      "\n",
      "leader size: 4000\n",
      "similarity time:  372.91960167884827\n",
      "MAP: 0.2657620101897219\n",
      "nDCG: 0.411918180112\n",
      "\n",
      "leader size: 5000\n",
      "similarity time:  461.2810027599335\n",
      "MAP: 0.26014166556335266\n",
      "nDCG: 0.405423213506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leader_size=[2000,3000,4000,5000]\n",
    "for i in leader_size:\n",
    "    print('leader size:',i)\n",
    "    cluster=pre_clustering(d_tfidf,i,0)\n",
    "    result=cosSimilarityImp(q_tfidf,cluster[0], cluster[1])\n",
    "    print('MAP:',MAP(result)) \n",
    "    print('nDCG:',mean_ndcg1(result))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leader size: 500\n",
      "similarity time:  69.35350036621094\n",
      "MAP: 0.21483842387504723\n",
      "nDCG: 0.365105603717\n",
      "\n",
      "leader size: 600\n",
      "similarity time:  73.99410009384155\n",
      "MAP: 0.22015704148524173\n",
      "nDCG: 0.372103523303\n",
      "\n",
      "leader size: 700\n",
      "similarity time:  83.1092004776001\n",
      "MAP: 0.21996053393525491\n",
      "nDCG: 0.371516094735\n",
      "\n",
      "leader size: 800\n",
      "similarity time:  88.52700066566467\n",
      "MAP: 0.2265230458335907\n",
      "nDCG: 0.379137753152\n",
      "\n",
      "leader size: 900\n",
      "similarity time:  97.65310049057007\n",
      "MAP: 0.23410509348729133\n",
      "nDCG: 0.384326388194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "leader_size=[500,600,700,800,900]\n",
    "for i in leader_size:\n",
    "    print('leader size:',i)\n",
    "    cluster=pre_clustering(d_tfidf,i,0)\n",
    "    result=cosSimilarityImp(q_tfidf,cluster[0], cluster[1])\n",
    "    print('MAP:',MAP(result)) \n",
    "    print('nDCG:',mean_ndcg1(result))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
